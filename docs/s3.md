# Performance Test Result storage in S3 bucket for SLV

**NOTE: This document is only relevant to puppet employees**

The SLV team has an s3 bucket for storing results from performance testing.

`
s3://slv-performance-results
`

## Convention

S3 is an
[object storage](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/using-folders.html) architecture,
so it doesn't have true directories, but it does like to make it look like it does.  The convention
is to use names like `foo/bar`  and `foo/car` to group files together.  They are really two separate
files that exist next to each other, but with the cli you can `ls foo/` and you will get both.

For our use, we will put results relating to a specific release
in `releases/<release_name>/<ticket>` Using the internal name like Kearney.

If it isn't specific to a release, put it in `tickets/<ticket>`

## General access
The bucket is public with static web hosting enabled. We also setup a modified version
of [S3 Bucket Browser](https://github.com/juvs/s3-bucket-browser) to enable normal browsing.
To access it you have to use a URL like this

http://slv-performance-results.s3-website-us-west-2.amazonaws.com/

That or anything that points to a folder will launch you into the Bucket Browser. If your URL
leads to an HTML or .txt file, it will get rendered.  And if it leads to any other file it
 will download it.

## Developer access/use

The bucket is owned by the Developer role for our ESO account which we should all have access
to.  Use that from the aws console to manipulate anything about how the bucket is setup.

### Install the aws cli

Follow the
[Installing AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html)
instructions for the machine you want to access s3 from.

### Credentials

Use the puppet-bastion account with the eso profile to assume the proper role
when connecting to AWS via the cli.  See
[confluence
docs](https://confluence.puppetlabs.com/pages/viewpage.action?pageId=104761111#AmazonWebServices\(AWS\)-Assumerolesviathecommandline)
for reference.

Create a `$HOME/.aws/credentials` file containing at least a default section.
```
[default]
aws_access_key_id = <personal id>
aws_secret_access_key = <personal key>
```

Create a `$HOME/.aws/config` file containing at least a default and eso profile section.
```
[default]
output = json
region = us-west-2

[profile eso]
role_arn = arn:aws:iam::<eso-account-number>:role/Developer
mfa_serial = arn:aws:iam::103716600232:mfa/<username>
source_profile = default
```

### CLI examples


List the top level of the bucket

`aws s3 ls s3://slv-performance-results/ --profile eso`

Copy release results to the bucket

`aws s3 cp --recursive /Users/bill.claytor/RubymineProjects/gplt/slv-425/PERF_1557340076 s3://slv-performance-results/releases/Kearney/SLV-425/PERF_1557340076 --profile eso`

List the releases (note the `/` after `releases`)

`aws s3 ls s3://slv-performance-results/releases/ --profile eso`


The `--profile` tells it which set of credentials to use.
The `--recursive` is for copying up directories, but be careful, because directories aren't a real
thing in s3.  You have to put the directory name in the destination, unlike linux cp.
Otherwise it copies the contents of your directory.

There is also a `cp`, `mv`, `rm`...
[Full CLI doc](https://docs.aws.amazon.com/cli/latest/reference/s3/index.html#cli-aws-s3)

## Automation use
When we start working on automation to use this, we will need to ensure we have credentials
on the testrunner, and then copy them to the host that will be accessing the bucket.  We will
also need to install the cli on the testrunner and the host.  There are libraries for various
programing languages, but since we will probably use a bolt task to trigger the access, just
shelling to the aws cli is probably
easiest.  This [AwsCli module](https://forge.puppet.com/jdowning/awscli) looks promising
for installing the cli.
